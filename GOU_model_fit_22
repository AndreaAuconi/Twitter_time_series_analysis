import os
os.chdir('/home/andrea/Desktop/Twitter_ITA_SciReps/2022/')
import numpy as np
import matplotlib.pyplot as plt
import pickle
from joblib import Parallel, delayed
import re
import json
from operator import itemgetter
from datetime import datetime
from datetime import timedelta
import powerlaw
from scipy.optimize import curve_fit
import multiprocessing
n_cores = int(multiprocessing.cpu_count()*0.85)

def clean_string(string):
    string = string.split('â€¦')[0]
    string = string.split('https')[0]
    cleaned_string = re.sub(r'[^A-Za-zÃ€-Ã¿]+', ' ', string)             
    string = ' '.join(string.split())    
    string = cleaned_string.lower()            
    return string

# %% Import from files

min_words = 2

tweets = []
directory = '/home/andrea/Desktop/Twitter_ITA_SciReps/2022/Data_22'
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        g = open(f, 'r')
        for line in g:
            tweets.append([json.loads(line)['timestamp_ms'],clean_string(json.loads(line)['text'])])

sorted_tweets = sorted(tweets, key=itemgetter(1))

tweets = []
for i in sorted_tweets:
    text = i[1]
    if (len(text.split()) >= min_words):
        tweets.append(i)

# %% Save Tweets

with open('/home/andrea/Desktop/Twitter_ITA_SciReps/2022/tweets_ITA22.ob', 'wb') as fp:
    pickle.dump(tweets, fp)

# %% Read tweets

with open ('/home/andrea/Desktop/Twitter_ITA_SciReps/2022/tweets_ITA22.ob', 'rb') as fp:
    tweets = pickle.load(fp)
    
# %% Build the Twitter mentions time series

keys_Calenda = set(['carlocalenda','calenda','azione'])
keys_Letta = set(['enricoletta','letta', 'pdnetwork'])
keys_Meloni = set(['giorgiameloni','meloni','fratelliditalia'])
keys_Conte = set(['giuseppeconteit','conte','stelle'])
keys_Renzi = set(['matteorenzi','renzi','italiaviva'])
keys_Salvini = set(['matteosalvinimi','salvini','lega','legasalvini'])
       
N = len(tweets)
n = int(N/n_cores)

tweets_subsets = []
for Id_subset in range(n_cores):
    start = Id_subset*n
    stop = (Id_subset+1)*n
    tweets_subsets.append(tweets[start:stop])

def records_mentions(these_tweets):    
    Calenda = []
    Letta = []
    Meloni = []
    Conte = []
    Renzi = []
    Salvini = []  
    this_n = len(these_tweets)
    previous = []
    for i in range(this_n):
        words = list(these_tweets[i][1].split())
        if words != previous:
            set_words = set(words)
            time = datetime.fromtimestamp(float(these_tweets[i][0])/1000)
            if not set_words.isdisjoint(keys_Calenda):
                Calenda.append(time)
            if not set_words.isdisjoint(keys_Letta):
                Letta.append(time)
            if not set_words.isdisjoint(keys_Meloni):
                Meloni.append(time)
            if not set_words.isdisjoint(keys_Conte):
                Conte.append(time) 
            if not set_words.isdisjoint(keys_Renzi):
                Renzi.append(time)
            if not set_words.isdisjoint(keys_Salvini):
                Salvini.append(time)
        previous = words               
    return [Calenda, Letta, Meloni, Conte, Renzi, Salvini]

results = Parallel(n_jobs=n_cores)(delayed(records_mentions)(tweets_subsets[Id_subset]) for Id_subset in range(n_cores))

t_Calenda = []; t_Letta = []; t_Meloni = []; t_Conte = []; t_Renzi = []; t_Salvini = [];
    
for i in results:
    for j in i[0]:
        t_Calenda.append(j)
    for j in i[1]:
        t_Letta.append(j)
    for j in i[2]:
        t_Meloni.append(j)
    for j in i[3]:
        t_Conte.append(j)       
    for j in i[4]:
        t_Renzi.append(j)
    for j in i[5]:
        t_Salvini.append(j)
       
t_Calenda = sorted(t_Calenda)
t_Letta = sorted(t_Letta)
t_Meloni = sorted(t_Meloni)
t_Conte = sorted(t_Conte)
t_Renzi = sorted(t_Renzi)
t_Salvini = sorted(t_Salvini)

parties = ['Calenda', 'Letta', 'Meloni', 'Conte', 'Renzi', 'Salvini']
time_series = [t_Calenda, t_Letta, t_Meloni, t_Conte, t_Renzi, t_Salvini]

# %% Functions and numerical parameters

# numerical parameters
sampling_timescale = timedelta(minutes=30) # larger than relaxation timescale to avoid inertia
further_detrend = False
delta_t = timedelta(seconds=30)
initialization = int(timedelta(hours=2)/sampling_timescale)
end_day, start_day = 1, 7 # night pause

# parameters for fitting and plotting
list_relaxation = [5, 10, 15, 20, 25, 30]
tau_0 = sampling_timescale # constraint, please only vary timescales
tau_max = timedelta(hours=30)
factor_tau_sampling = 1.2

# Auxiliary variables
day_hours = [i for i in range(start_day,24)] + [i for i in range(end_day)]
min_t = min([i[0] for i in time_series])
max_t = datetime(2022, 9, 23, 18, 0, 0, 0) #pre-election silence
n = int(1 + np.log(tau_max/tau_0)/np.log(factor_tau_sampling))
int_tau_list = list(set([int(np.power(factor_tau_sampling,i)) for i in range(0,n)]))
n_points = len(int_tau_list)
tau_0_hours = tau_0.seconds/np.power(60,2)
tau_list = [tau_0_hours*i for i in int_tau_list]

def isDay(hour):
    return hour in day_hours

def low_pass_filter(events_list,relaxation_timescale):
    beta = 1./relaxation_timescale.seconds
    relaxation = np.exp(-delta_t/relaxation_timescale)
    x_list, t_list = [], []
    x, t = 0., min_t
    recording = isDay(min_t.hour)
    for i in events_list:
        if i > min_t and i < max_t:
            if recording and (not isDay(i.hour)):
                recording = False
                while t.hour < end_day:
                    t += delta_t
                    x = x*relaxation
                    x_list.append(x)
                    t_list.append(t)
            elif isDay(i.hour):
                if not recording:
                    recording = True             
                    t = datetime(i.year, i.month, i.day, start_day, 0, 0, 0)
                tau = int((i-t)/delta_t)
                for _ in range(tau):
                    t += delta_t
                    x = x*relaxation
                    x_list.append(x)
                    t_list.append(t)
                x += beta
    while t < max_t:
        t += delta_t
        x = x*relaxation
        x_list.append(x)
        t_list.append(t)
    sampling = int(sampling_timescale/delta_t)
    n = int(len(x_list)/sampling)   
    x_sample = [x_list[i*sampling] for i in range(initialization,n)]
    t_sample = [t_list[i*sampling] for i in range(initialization,n)]
    return x_sample, t_sample

def exp_fun(this_t,a,b):
    return a*np.exp(b*this_t)

def exp_detrend(series):
    factor = 1e-4
    t_disc = [factor*i for i in range(len(series))]
    [a,b],_ = curve_fit(exp_fun,t_disc,series)
    new_list = [series[i]/exp_fun(t_disc[i],a,b) for i in range(len(series))]
    return new_list, a, b

def E_s(T,drift,volatility):
    return (np.power(volatility,2)/drift)*(1-np.exp(-drift*T))

def GOU_analysis(to_fit): 
    [series, Poisson] = to_fit
    dx2_series = []
    curve = []
    for int_tau in int_tau_list:
        this_len = len(series)-int_tau
        dx2 = np.power(np.log(np.roll(series,-int_tau)/series),2)[:this_len]
        dx2_series.append(dx2)
        curve.append(np.mean(dx2)-Poisson)              
    [drift, volatility],_ = curve_fit(E_s, tau_list, curve, p0 = [0.1,0.1], maxfev = 10000)
    normalized_fit = [[item*drift for item in tau_list],\
                      [item/(np.power(volatility,2)/drift) for item in curve]]   
    normalized_dx2 = []
    for i in range(n_points):
        tau = tau_list[i]
        Z = E_s(tau,drift,volatility)
        normalized_dx2.append([item/Z for item in dx2_series[i]])    
    return normalized_fit, normalized_dx2, [drift, volatility]

# %% Analysis

correlation_time = []
    
for rel in list_relaxation:

    relaxation_timescale = timedelta(minutes=rel)
    
    ## %%  Low-pass filter and normalization      
    filtered = Parallel(n_jobs=n_cores)(delayed(low_pass_filter)(i,relaxation_timescale) for i in time_series)       
    x_list_raw, t_list = [i[0] for i in filtered], filtered[0][1]  
    trend = np.mean(x_list_raw,axis=0)
    rates = np.mean(x_list_raw,axis=1)    
    x_list = x_list_raw/(len(parties)*trend) # detrended        
    if further_detrend:
        x_list = [exp_detrend(i)[0] for i in x_list]
    
    ## %% estimation Poisson noise    
    Var_rates = np.var(x_list_raw,axis=1)
    Poisson_Var = (1./(relaxation_timescale.seconds*rates))*(1 + Var_rates/np.power(rates,2))
    
    if rel == 20:
        x_list_plots = x_list
        Poisson_plots = Poisson_Var
        
    ## %%  GOU calibration        
    data_to_fit = [[x_list[i], Poisson_Var[i]] for i in range(len(parties))]               
    analysis = Parallel(n_jobs=n_cores)(delayed(GOU_analysis)(this) for this in data_to_fit)      
    for i in range(len(parties)):
        print(parties[i])
        print(analysis[i][2])
    
    ## %% Plot normalized GOU fit    
    tau_list_refined = [tau_0_hours*np.power(2,i/100) for i in range(-500,700)]
    log_normal_scaling = [E_s(t,1.,1.) for t in tau_list_refined]
    GBM_scaling = [t for t in tau_list_refined]   
    plt.clf()
    for this in analysis: 
        plt.scatter(this[0][0],this[0][1],s=50)
    plt.plot(tau_list_refined, log_normal_scaling, linewidth = 1.5, linestyle = 'dashed', color = 'black')
    plt.plot(tau_list_refined, GBM_scaling, linewidth = 1.5, linestyle = 'dashed', color = 'gray')
    names = parties + ['GOU fit', 'GBM']
    plt.legend(names)
    plt.ylim(0.,1.15)
    plt.xlim(0.,7.5)
    plt.ylabel(r'$\mathbb{E} z(\tau)$      ', rotation = 0, fontsize=15)
    plt.xlabel(r'$\tau$', fontsize=14)
    filename = '22_Normalized_fit_' + str(relaxation_timescale.seconds/60.)[:5] + '.pdf'
    plt.tight_layout()
    plt.savefig(filename, bbox_inches='tight')
        
    ## %% Plot distributions p(z) for each time interval    
    if rel == 20:
        n_replicas = 1e7
        log_normal_process = []
        for i in range(int(n_replicas)):
            log_normal_process.append(np.power(np.random.normal(0,1),2))               
        for i in range(n_points):
            tau = tau_list[i]
            plt.clf()
            for j in range(len(parties)): 
                powerlaw.plot_pdf(analysis[j][1][i])
            names = parties + ['GOU fit']
            powerlaw.plot_pdf(log_normal_process, linewidth = 1.5, linestyle = 'dashed', color = 'black')
            plt.legend(names)
            plt.xlim(2e-5, 70)
            plt.ylim(8e-6, 1.5e3)
            plt.ylabel(r'$p(\widetilde{z})$', rotation = 0, fontsize=14)
            plt.xlabel(r'$\widetilde{z}$', fontsize=14)
            filename = 'p_z/22_Distribution_tau_' + str(tau)[:6] + '.pdf'
            plt.tight_layout()
            plt.savefig(filename, bbox_inches='tight')
    
    ## %% Estimated timescale 
    corr_time = np.sum([rates[i]*(1/analysis[i][2][0])/np.sum(rates) for i in range(len(parties))])
    correlation_time.append(corr_time)

plt.clf()
plt.scatter(list_relaxation,correlation_time)
plt.xlabel(r'$\beta^{-1}$ [min]', size = 14)
plt.ylabel(r'$\alpha^{-1}$ [hours]', size = 14)
plt.ylim(0,15)
plt.savefig('22_timescale_estimation.pdf')
